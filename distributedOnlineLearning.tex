
\subsubsection*{Distributed Online Learning}
\par In recent years, the problem of distributed online learning has received significant attention and has been studied in \cite{langford2009slow,yan2013distributed,xiao2010dual,dekel2012optimal,kamp2014communication}.  A distributed online mini-batch prediction approach over multiple data streams has been proposed in \cite{dekel2012optimal}. This approach is based on a static synchronization method. The learners periodically communicate  their local models with a central coordinator unit after consuming a fixed number of input samples/events (i.e., batch size $b$), in order to  create a global model and share it between all learners. This work has been extended in \cite{kamp2014communication} by introducing a
dynamic synchronization scheme that reduces the required communication overhead. It can do so by making the local learners communicate their models only if they diverge form a reference model. In this work, we aim to employee this protocol with event patterns prediction models over multiple event streams. 
