

\section{IMPLEMENTATION DETAILS}
\label{sec:impl}
This sections provides a detailed overview of the system's implementation, our system has been implemented on top of Apache Flink and Apache Kafka frameworks. Each of the three sub-modules described in Section  ~\ref{sec:architecture} have been implemented as Flink operations over the input events stream. 

\textbf{Pre-processing and Prediction Operators.} Listing ~\ref{algonline:flink1} shows how the workflow of the system is implemented as data flow in Flink program.


	\begin{lstlisting}[caption={Flink pipeline for local predictors workflow},label={algonline:flink1},frame=single]
	.....
	DataStream<Event> inputEventsStream = env.addSource(flinkKafkaConsumer);	
	// Create event tuples (id,event) and assign time stamp 
	DataStream<Tuple2<String, sEvent>> eventTuplesStream =
	inputEventsStream.map(new EventTuplesMapper())
	.assignTimestampsAndWatermarks(new EventTimeAssigner());	
	// Create the ordered keyed stream 
	KeyedStream<Tuple2<String, Event>, Tuple> keyedEventsStream =
	eventsStream.keyBy(0).process(new EventSorter())
	.keyBy(0);	
	//Initialize the predictor node 
	LocalPredictorNode predictorNode =new LocalPredictorNode<Event>(P);
	// Process the  keyedEventsStream by the predictor 
	DataStream<Event> processedEventsStream =
	keyedEventsStream.map(predictorNode);
	...
	\end{lstlisting}
	
The system ingests the input events stream form a Kafka cluster that mapped to \textbf{DataStream} of events, which is then processed by a \textbf{EventTuplesMapper} to create tuples of (id,event), where the  \textbf{id} is associated moving object id. And in order to manage the out of order incoming events the tuples stream is processed by a \textbf{TimestampAssigner} that assigns the timestamps for the input events  based on the extracted creation time. Afterward,  an ordered stream of event tuples is generated using a process function \textbf{EventSorter}. The ordered stream is transformed to a \textbf{keyedEventsStream} based on partitioning it based on the ids of the associated moving object using a \textbf{keyBy} operation. A local \textbf{predictor} node in a distributed environment is represented by a \textbf{map} function over the \textbf{keyedEventsStream}, while each parallel instance of the map operator (predictor) always processes all events with the same id, and maintains a bounded prediction model (i.e., pattern markov chain predictor) using the Flink's Keyed State \footnote{\url{https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/stream/state.html\#keyed-state}}.   The output streams of the moving objects form the predictor map parallel instances are sent to a new Kafka stream (i.e., same topic name) that can be processed by other components like visualization or users notifier.


\par Moreover, the implementation of  a \textbf{predictor} map function includes the communication  with \textbf{coordinator} using Kafka streams. At the beginning of the execution it sends a reregistration request to the coordinator. And at runtime it sends  its local prediction model as synchronization request,  or as a response for a resolution request from the coordinator. In addition, it receives a resolution request from the coordinator to send its model. These communication messages are published onto different  Kafka topics as depicted in Table~\ref{tab:messagesToTopics}. 

\begin{table}[h]
	\caption{Message to Kafka topics mapping.}
	\label{tab:messagesToTopics}
	\begin{tabular}{p{3cm}l}
		\toprule
		Message &Kafka Topic\\
		\midrule
		\parbox[t]{4cm}{\textbf{RegisterNode}, \\ \textbf{RequestSync} and \\\textbf{ResolutionAnswer} } &LocalToCoordinatorTopicId\\ \\
		
			  \parbox[t]{4cm}{\textbf{CoordinatorSync} and \\ \textbf{RequestResolution}} &CoordinatorToLocalTopicId\\
		
		\bottomrule
	\end{tabular}
\end{table}


\textbf{Coordinator.} Listing ~\ref{algonline:flink2} presents the workflow of the coordinator node that manages the distributed online learning protocol operations, which is implemented as Flink program. The coordinator receives messages from the local predictors through a Kafka Stream of a topic named \textbf{"LocalToCoordinatorTopicId"}, while it is implemented as a single \textbf{map} function over the messages stream by setting the \textbf{parallelism} level of the Flink program to \textbf{"1"} \footnote{Increasing the parallelism will scale up the number of parallel coordinator instances, but for the current setting a single node in needed.}. The coordinator map operator handles three type of messages from the predictors: \begin{enumerate*}[(i)]
	\item \textbf{RegisterNode} that contains  a registration request for a new predictor instance,
	\item \textbf{RequestSync} to receive a local model of a predictor node after violation,
	\item \textbf{ResolutionAnswer} to receive a resolution response from  a local predictor node.  
\end{enumerate*}  
 While it sends \textbf{CoordinatorSync} messages for all predictors after creating a new global prediction model, or \textbf{RequestResolution} to a ask the local predictors for their prediction models.
 

\begin{lstlisting}[caption={The coordinator Flink program.},label={algonline:flink2},frame=single]
...
	StreamExecutionEnvironment env = [...];
	// Set a default parallelism for all operators 
	env.setParallelism(1);
 
	// Read messages from local predictors
	DataStream<TopicMessage> messagesStream = readKafkaStream(env, "LocalToCoordinatorTopicId");
	
	// Initialize the coordinator node
	CompunctionEfficientCoordinator coordinatorNode = new CompunctionEfficientCoordinator(configs);
	
	DataStream<CoordinatorMessage> coordinatorMessagesStream = messageStream.map(coordinatorNode);
	
	// Send the messages form the coordinator to the local predictors
	writeKafkaStream(coordinatorMessagesStream, CoordinatorToLocalTopicId);

...
\end{lstlisting}

                 