\frame
{
  \frametitle{ Event Forecasting with Pattern Markov
 	Chains  }
	\framesubtitle{\citep{alevizos2017event}}
  \begin{itemize}[]
  	\item<1-> The system consumes a single input stream $s_i$ of events.
  	\item<1-> The event stream $s_i$ is assumed to be generated by  $m$-order Markov source.
  \item<1-> The complex event (i.e., pattern) $P$ is defined in the form of regular expressions over a finite set of event types $\Sigma$.

  \item<1-> A probabilistic model provides online forecasting reports when the event pattern $P$ is expected to be completed in future. 
   
  \end{itemize}
}

\frame
{
	\frametitle{Event Forecasting with Pattern Markov
		Chains  }
	\framesubtitle{ How does it work?}
		\begin{itemize}
		\item<only@1> The pattern $P=Sailing\cdot Stopping$ is converted to $DFA$ with $\Sigma=\{Moving,Sailing,Stopping\}$. 
		\include{dfa_example}
	
			\end{itemize}
}



\frame
{
	\frametitle{Event Forecasting with Pattern Markov
		Chains}
	\framesubtitle{How does it work?}
	\begin{itemize}
		\item<only@1> The  deterministic finite automa ($DFA$) is used to construct a Markov chain, which is called a Pattern Markov Chain (PMC).
		
		
	    \item<only@1> The states of $DFA$ is directly mapped to states of  transition probability matrix $\boldsymbol{M}$  $\lvert Q \rvert \times \lvert Q \rvert$ of the $PMC$.
		
		\item<only@1> 
	\begin{equation*}
	\label{eq:matrix_example}
	\boldsymbol{M} = 
	\begin{Bmatrix} 
	0 \\ 1 \\ 2 \\ 3 \\4
	\end{Bmatrix}
	\begin{pmatrix} 
	p_{0,0}	    &. 		&. 		& . &  	p_{0,4} \\
    . 		    & .		& .	& .	& . \\
	.		    & .		& .		& .	& . \\
	.			& .		& .		& .	& .\\
	0			& .			& .		& .	&p_{4,4}
	\end{pmatrix}
	\end{equation*}
	
	\item<only@1> The maximum-likelihood estimator is used to compute the transition probabilities (i.e., learning) $p_{i,j}$ of the matrix $\boldsymbol{M}$ 
	\begin{equation}
	\label{eq:pi_estim}
	\hat{p}_{i,j}=\frac{n_{i,j}}{\sum_{k \in Q} n_{i,k}}=\frac{n_{i,j}}{n_{i}}
	\end{equation}. 	
		
	\end{itemize}
}



\frame
{
	\frametitle{Event Forecasting with Pattern Markov
		Chains}
	\framesubtitle{How does it work?}
	\begin{itemize}
		
		\item<only@1> The probability distribution of the waiting-time (i.e., time required until the pattern is completed from state $i$) $P(W_{P}(i)=n)$, is calculated based on the Markov chain transition matrix. 
	
		\begin{equation*}
		P(W_{P}(i)=n)=\boldsymbol{\xi_{i}}^{T}\boldsymbol{N}^{n-1}(\boldsymbol{I}-\boldsymbol{N})\boldsymbol{1} 
		\end{equation*}
		where 
		\begin{equation}
		\label{eq:matrix}
		\boldsymbol{M} = 
		\begin{pmatrix} 
		\boldsymbol{N} & \boldsymbol{C}  \\ 
		\boldsymbol{0} & \boldsymbol{I}
		\end{pmatrix}
		\end{equation}
		\item<only@1> The forecast intervals $I=(start,end)$ (i.e., the completion interval of the pattern $P$ from the  current state $q$) are built using the waiting-time distribution $$P(I)=\sum_{n \in I}{P(W_{P}(q)=n)} \quad \textrm{and} \quad P(I) \geq \mathit{prediction \_ threshold}\ $$. 
			
	\end{itemize}
}



\frame
{
	\frametitle{Event Forecasting with Pattern Markov
		Chains}
	\framesubtitle{What is wrong?}
	\begin{itemize}
		
		\item<only@1> The overhead learning time of the Markov transition matrix for each input stream isolated from the others, how can we accelerate it?  
		 
		\item<only@1> The low performance of the model for an input event stream with less data. 
		
			\item<only@1> How can we enable the communication between the local models 
			 and leverage an aggregated global shared model in 
		 	a distributed and communication efficient fashion while maintaining
			provable and strong guarantees in service quality?
			
	
	\end{itemize}
}
